# Data-Engineering---ETL-Data-Pipelines-with-Bash-and-Airflow

#### ETL (extration, transform and loading) process enable organizations to leverage the power of their data to provide information, intelligence and actionable insights into the organization's processes.
ETL data pipelines allows for the curation of data from multiple sources, transform the data into some other formats or structure and then use the transformed data for other other downstream applications. The data can also be loaded onto a repository for some other purpose. With ETL, organizations can easily track and govern their entire data transformation and application processes. 

Apache AIrflow is an ETL platform that can be programmatically used to set up pipelines or workflows for data ETL applications. This tutorial highlights how to set up ETL data pipelines using Airflow and Bash (BashOperator). The tutorial completely solves the final ETL assessment segment in the Coursera's IBM Data Pipeline with Shell, Airflow and Kafka (https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/module/1) course

## PROJECT OVERVIEW
